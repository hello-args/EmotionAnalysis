{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/big_train_test/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['content', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'labels'. \n",
    "train_df['sentiment_labels']= label_encoder.fit_transform(train_df['sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 10,  3,  8, 12, 11,  7,  4,  6,  5,  1,  9,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment_labels'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform(train_df['sentiment_labels'].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df['sentiment'] = train_df['sentiment'].astype('category')\n",
    "train_df['sentiment'] = train_df['sentiment'].cat.codes\n",
    "dict( zip( train_df['sentiment'].cat.codes, train_df['sentiment'] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "anger           94\n",
       "boredom        154\n",
       "empty          588\n",
       "enthusiasm     486\n",
       "fun           1008\n",
       "happiness     2742\n",
       "hate          1136\n",
       "love          1893\n",
       "neutral       5566\n",
       "relief         967\n",
       "sadness       4575\n",
       "surprise      1495\n",
       "worry         7027\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['sentiment']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.sentiment_labels != 0]\n",
    "train_df = train_df[train_df.sentiment_labels != 1]\n",
    "train_df = train_df[train_df.sentiment_labels != 2]\n",
    "train_df = train_df[train_df.sentiment_labels != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-1,13):\n",
    "    len_s = len(train_df.loc[train_df['sentiment_labels'] == i])\n",
    "    if len_s > 1500:\n",
    "        diff = len_s-1500\n",
    "        to_remove = np.random.choice(train_df[train_df['sentiment_labels']==i].index,size=diff,replace=False)\n",
    "        train_df = train_df.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "fun          1008\n",
       "happiness    1500\n",
       "hate         1136\n",
       "love         1500\n",
       "neutral      1500\n",
       "relief        967\n",
       "sadness      1500\n",
       "surprise     1495\n",
       "worry        1500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['sentiment']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELPING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from itertools import islice\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets(\"everyone\")\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "json_path = 'data/text_alternatives.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "abb_json = json_data['abb_json']\n",
    "smiley = json_data['smiley']\n",
    "smiley_list = smiley.keys()\n",
    "\n",
    "from itertools import groupby \n",
    "from string import punctuation\n",
    "punc = set(punctuation)\n",
    "\n",
    "subst_string = {'\\'ve': ' have', '`ve': ' have', '\\'s': '', '`s': '', 'n\\'t': ' not', \n",
    "                'n`t': ' not', '\\'ll': ' will', '`ll': ' will', '\\'d': ' had', \n",
    "                '`d': ' hd', '\\'re': ' are', '`re': ' are'}\n",
    "\n",
    "\n",
    "def remove_cons_punct(text):\n",
    "    newtext = []\n",
    "    for k, g in groupby(text):\n",
    "        if k in punc:\n",
    "            newtext.append(k)\n",
    "        else:\n",
    "            newtext.extend(g)\n",
    "\n",
    "    return ''.join(newtext)\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "\n",
    "smiley_list = list(chunks(list(smiley_list), 10))\n",
    "abb_list = abb_json.keys()\n",
    "abb_list = [nlp(text) for text in abb_list]\n",
    "matcher.add('abb_list', None, *abb_list)\n",
    "try:\n",
    "    for each_list in smiley_list:\n",
    "        each_list = [nlp(text) for text in each_list]\n",
    "        matcher.add('smiley_list', None, *each_list)\n",
    "except:\n",
    "    print(\"exception occured while adding patterns\")\n",
    "    \n",
    "    \n",
    "def remove_hashtag_mentions(text):\n",
    "    text = re.sub(r'((?<=^)([(@|#)][^  ]+)|(?<= )([(@|#)][^ ]+))', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_html_texts(text):\n",
    "    text = re.sub(r'(&.+?;)', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'https?://.*[\\r\\n]*', '', text)\n",
    "    return text\n",
    "\n",
    "def process_tweets_for_abb_smiley(text):\n",
    "    text = re.sub(r'(?=([a-z])\\1\\1).', '', text)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        if doc[start:end].text in abb_json.keys():\n",
    "            text = text.replace(doc[start:end].text, abb_json[doc[start:end].text])\n",
    "        if doc[start:end].text in smiley.keys():\n",
    "            text = text.replace(doc[start:end].text, smiley[doc[start:end].text])\n",
    "    return remove_cons_punct(text.lower())\n",
    "\n",
    "\n",
    "def process_abbword_smiley_links_hashtags_mentions(text, remove_links_urls=True, remove_hashtag_and_mentions=True):\n",
    "    text = process_tweets_for_abb_smiley(text)\n",
    "    text = remove_html_texts(text)\n",
    "    if remove_links_urls:\n",
    "        text = remove_url(text)\n",
    "    if remove_hashtag_and_mentions:\n",
    "        text = remove_hashtag_mentions(text)\n",
    "    return text\n",
    "\n",
    "def get_string_removing_consequtive_similar_char(text):\n",
    "    new_text_list = text.split(\" \")\n",
    "    new_string = \"\"\n",
    "    for new_text in new_text_list:\n",
    "        meaningful = []\n",
    "        if len(wordnet.synsets(new_text)) ==0:\n",
    "            consq_char = re.findall(r'([a-z])\\1', new_text)\n",
    "            consq_char_comb = get_char_combination(consq_char)\n",
    "            for comb in consq_char_comb:\n",
    "                pattern = r'(?=([' + re.escape(comb) + r'])\\1).'\n",
    "                new_word = re.sub(pattern, \"\", new_text)\n",
    "                if wordnet.synsets(new_word):\n",
    "                    meaningful.append(new_word)\n",
    "            if meaningful:\n",
    "                new_string += max(meaningful, key=len) + \" \"\n",
    "                \n",
    "            else: new_string += new_text + \" \"\n",
    "        else:\n",
    "            new_string += new_text + \" \"\n",
    "    return new_string\n",
    "\n",
    "def get_char_combination(consq_char):\n",
    "    n = len(list(consq_char))\n",
    "    arr = [None] * n\n",
    "    binary_arr = generateAllBinaryStrings(n, arr, 0, [])\n",
    "    result = []\n",
    "    for elem in binary_arr:\n",
    "        c = \"\"\n",
    "        c = \"\".join([c + elem[i]*consq_char[i] for i in range(0,n)])\n",
    "        if c != \"\":\n",
    "            result.append(c)\n",
    "        else: continue\n",
    "        \n",
    "    return result\n",
    "\n",
    "def generateAllBinaryStrings(n, arr, i, m_arr):  \n",
    "  \n",
    "    if i == n: \n",
    "        m_arr.append(arr[:])  \n",
    "        return m_arr\n",
    "      \n",
    "    # First assign \"0\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 0\n",
    "    m_arr = generateAllBinaryStrings(n, arr, i + 1, m_arr)  \n",
    "  \n",
    "    # And then assign \"1\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 1\n",
    "    m_arr = generateAllBinaryStrings(n, arr, i + 1, m_arr)  \n",
    "    return m_arr\n",
    "\n",
    "\n",
    "def get_lemma_from_string(text, remove_stopwords=True, remove_punctuation=False):\n",
    "    lemmatised_input_list_string = nlp(u'' + text)\n",
    "    lemmatised_list = list()\n",
    "    for token in lemmatised_input_list_string:\n",
    "        if not token.lemma_ == '-PRON-':\n",
    "            if remove_punctuation and remove_stopwords:\n",
    "                if not token.is_stop and (token.text == '.' or not token.is_punct):\n",
    "                    lemmatised_list.append(token.lemma_)\n",
    "            elif remove_punctuation:\n",
    "                if token.text == '.' or not token.is_punct:\n",
    "                    lemmatised_list.append(token.lemma_)\n",
    "            elif remove_stopwords:\n",
    "                if not token.is_stop:\n",
    "                    lemmatised_list.append(token.lemma_)\n",
    "    lemmatised_string = \" \".join(lemmatised_list)\n",
    "    return lemmatised_string.strip()\n",
    "\n",
    "def replace_string(df):\n",
    "    for text in subst_string.keys():\n",
    "        df['content_lemma'] = df['content_lemma'].str.replace(text, subst_string[text])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['content'] = train_df.apply (lambda row: process_abbword_smiley_links_hashtags_mentions(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content'] = train_df.apply (lambda row: get_string_removing_consequtive_similar_char(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content_lemma'] = train_df.apply (lambda row: get_lemma_from_string(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = replace_string(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content_len'] = train_df['content'].str.len()  # Store string length of each sample\n",
    "train_df['content_lemma_len'] = train_df['content_lemma'].str.len()  # Store string length of each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_labels</th>\n",
       "      <th>content_lemma</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_lemma_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>charlene my love. i miss you</td>\n",
       "      <td>sadness</td>\n",
       "      <td>10</td>\n",
       "      <td>charlene love . miss</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>got the news</td>\n",
       "      <td>surprise</td>\n",
       "      <td>11</td>\n",
       "      <td>get news</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>the storm is here and the electricity is gone</td>\n",
       "      <td>sadness</td>\n",
       "      <td>10</td>\n",
       "      <td>storm electricity go</td>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>agreed</td>\n",
       "      <td>love</td>\n",
       "      <td>7</td>\n",
       "      <td>agree</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>so sleepy again and it's not even that late. i...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>10</td>\n",
       "      <td>sleepy late . fail .</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28934</td>\n",
       "      <td>hanging with my cousin jimmy then hopefully ha...</td>\n",
       "      <td>fun</td>\n",
       "      <td>4</td>\n",
       "      <td>hang cousin jimmy hopefully hang friend</td>\n",
       "      <td>67</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28936</td>\n",
       "      <td>with alex</td>\n",
       "      <td>sadness</td>\n",
       "      <td>10</td>\n",
       "      <td>alex</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28937</td>\n",
       "      <td>that is comedy  good luck my friend!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>5</td>\n",
       "      <td>comedy   good luck friend !</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28938</td>\n",
       "      <td>stephs grad party gr8! shoved cake in her face...</td>\n",
       "      <td>fun</td>\n",
       "      <td>4</td>\n",
       "      <td>stephs grad party gr8 ! shoved cake face   wat...</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28939</td>\n",
       "      <td>sweet - san fran is awesome!  love it there</td>\n",
       "      <td>happiness</td>\n",
       "      <td>5</td>\n",
       "      <td>sweet - san fran awesome !   love</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12106 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  sentiment  \\\n",
       "8                          charlene my love. i miss you     sadness   \n",
       "14                                         got the news    surprise   \n",
       "15        the storm is here and the electricity is gone     sadness   \n",
       "16                                               agreed        love   \n",
       "17     so sleepy again and it's not even that late. i...    sadness   \n",
       "...                                                  ...        ...   \n",
       "28934  hanging with my cousin jimmy then hopefully ha...        fun   \n",
       "28936                                         with alex     sadness   \n",
       "28937              that is comedy  good luck my friend!   happiness   \n",
       "28938  stephs grad party gr8! shoved cake in her face...        fun   \n",
       "28939       sweet - san fran is awesome!  love it there   happiness   \n",
       "\n",
       "       sentiment_labels                                      content_lemma  \\\n",
       "8                    10                               charlene love . miss   \n",
       "14                   11                                           get news   \n",
       "15                   10                               storm electricity go   \n",
       "16                    7                                              agree   \n",
       "17                   10                               sleepy late . fail .   \n",
       "...                 ...                                                ...   \n",
       "28934                 4            hang cousin jimmy hopefully hang friend   \n",
       "28936                10                                               alex   \n",
       "28937                 5                        comedy   good luck friend !   \n",
       "28938                 4  stephs grad party gr8 ! shoved cake face   wat...   \n",
       "28939                 5                  sweet - san fran awesome !   love   \n",
       "\n",
       "       content_len  content_lemma_len  \n",
       "8               29                 20  \n",
       "14              13                  8  \n",
       "15              46                 20  \n",
       "16               7                  5  \n",
       "17              64                 20  \n",
       "...            ...                ...  \n",
       "28934           67                 39  \n",
       "28936           10                  4  \n",
       "28937           37                 27  \n",
       "28938          102                 96  \n",
       "28939           44                 33  \n",
       "\n",
       "[12106 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------IF YOU NEED TO SPLIT DF-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fsplit.filesplit import FileSplit\n",
    "fs = FileSplit(file='new_data/train_df.csv', splitsize=50000, output_dir='new_data/splits/')\n",
    "fs.split(include_header=True)\n",
    "from os import walk\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk('new_data/splits/'):\n",
    "    for file in filenames:\n",
    "        df = pd.read_csv('new_data/splits/'+file)\n",
    "        df['content'] = df.apply (lambda row: get_string_removing_consequtive_similar_char(row['content']), axis=1)\n",
    "        print('done', file)\n",
    "        df.to_csv('new_data/cleaned/'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/new_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
